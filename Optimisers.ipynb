{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pressure Vessel Optimisation ##\n",
    "\n",
    "There are 4 variables that describe a pressure vessel:\n",
    "z1 - Shell thickness\n",
    "z2 - Head thickness\n",
    "x3 - Inner radius\n",
    "x4 - vessel length (excluding head)\n",
    "\n",
    "Our input variables:\n",
    " x1,x2 = 0,0\n",
    "\n",
    "Decision Vector:\n",
    "np.array(x1,x2,x3,x4)\n",
    "\n",
    "\n",
    "\n",
    "The shell head and thickness can be expressed as\n",
    "z1 = 0.0625 * x1\n",
    "z2 = 0.0625 * x2\n",
    "\n",
    "# Bounds:\n",
    "x1, x2 are integers between 1 and 99 \n",
    "x3, x4 are continuous values between 10 and 200\n",
    "\n",
    "# Best known feasible solution of f(x*) = 5.8853327736Ã—10^3\n",
    "\n",
    "# Questions:\n",
    "1: I chose to implement Simulated Annealing with exponential decay as my stochastic optimiser.\n",
    "\n",
    "2: I chose this optimiser because: <br>\n",
    " * Compared to other stochastic optimisers we've looked at, it is simple to implement and adaptible ie it can fairly easily be made to fit almost any problem!. <br>\n",
    " * Exponential decay yielded the best results during my initial testing. <br>\n",
    " * Simulated annealing is good in situations when there are many local minima separated by peaks, the inital focus on exploration means that we take a chance to explore regions that appear to have worse results (but may in fact have a lower minimum lying beyond!) and then we exploit this knowledge to find an optimum<br>\n",
    "\n",
    "3: I am using both static penalty and death penalty. <br>\n",
    "    I initally only implemented death penalty because: <br>\n",
    "    * It is simple to implement in order to get quick results. <br>\n",
    "    * It doesn't require tuning like static penalty does.\n",
    "I then extended my program to use static penalty because: <br>\n",
    "* This problem has a fairly small feasible space, death penalty is not likely to help us close in on it. <br>\n",
    "* Static penalty is not naive, meaning that it uses information from the results as we go along, enabling us to get closer to feasible results <br>\n",
    "* Allows the use of tuning variables to adapt it to the optimisation problem at hand <br>\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import truncnorm, ttest_ind\n",
    "\n",
    "np.random.seed(13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise counters\n",
    "# These should be no more than 336,000 by the end (4000 x 2 for both optimisers, x 2 for comparing static and death penalty and x 21 repetitions)\n",
    "f_call_count = 0\n",
    "g1_call_count = 0\n",
    "g2_call_count = 0\n",
    "g3_call_count = 0\n",
    "g4_call_count = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g1(xs):\n",
    "    global g1_call_count\n",
    "    g1_call_count += 1\n",
    "    return xs[2]*0.00954\n",
    "    \n",
    "\n",
    "def g2(xs):\n",
    "    global g2_call_count\n",
    "    g2_call_count += 1\n",
    "    return xs[2]*0.0193\n",
    "    \n",
    "        \n",
    "def g3(xs):\n",
    "    global g3_call_count\n",
    "    g3_call_count += 1\n",
    "    return xs[3]\n",
    "  \n",
    "\n",
    "def g4(xs):\n",
    "    global g4_call_count\n",
    "    g4_call_count += 1\n",
    "    return (-1)*(np.pi)*(xs[2]**2)*(xs[3]) - ((4/3)*np.pi)*(xs[2]**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_constraints(xs):\n",
    "    z1, z2 = calculate_zs(xs)\n",
    "    # penalty_sum is the total magnitude by which a solution violates the constraints.\n",
    "    penalty_sum = 0\n",
    "    g1_out = g1(xs)\n",
    "    g2_out = g2(xs)\n",
    "    g3_out = g3(xs)\n",
    "    g4_out = g4(xs)\n",
    "\n",
    "    if g1_out <= z2:\n",
    "        penalty_sum += s1(z2, g1_out)\n",
    "    if g2_out <= z1:\n",
    "        penalty_sum += s2(z1, g2_out)\n",
    "    if g3_out <= 240:\n",
    "        penalty_sum += s3(g3_out)\n",
    "        \n",
    "    if g4_out <= -12960000:\n",
    "        penalty_sum += s4(g4_out)\n",
    "    \n",
    "    return penalty_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s1(z2, g1):\n",
    "    return abs(g1-z2)\n",
    "\n",
    "def s2(z1, g2):\n",
    "    return abs(g2-z1)\n",
    "    \n",
    "\n",
    "def s3(g3):\n",
    "    return abs(g3-240)\n",
    "\n",
    "def s4(g4):\n",
    "    return abs(g4 - (-1296000))\n",
    "\n",
    "def static_penalty(fx, xs):\n",
    "    beta = 2\n",
    "    r = 50\n",
    "    m = 1000\n",
    "\n",
    "    xout = fx + r*(check_constraints(xs))**beta + m\n",
    "    return xout\n",
    "\n",
    "def death_penalty(fx, xs):\n",
    "    if (check_constraints(xs) == 0):\n",
    "        return fx\n",
    "    else:\n",
    "         return math.inf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_zs(xs):\n",
    "    z1 = (xs[0]*0.0625)\n",
    "    z2 = (xs[1]*0.0625)\n",
    "    return z1, z2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(xs):\n",
    "    global f_call_count\n",
    "    f_call_count += 1\n",
    "\n",
    "    z1, z2 = calculate_zs(xs)\n",
    "\n",
    "    x3 = xs[2]\n",
    "    x4 = xs[3]\n",
    "\n",
    "    xnew = (1.7781)*(z2)*(x3**2) + (0.6224)*(z1)*(x3)*(x4) + (3.1661)*(z1**2)*(x4) + (19.84)*(z1**2)*(x3)\n",
    "    return xnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    xs = np.array([1, 35, 11.678, 88.76])\n",
    "    z1,z2 = calculate_zs(xs)\n",
    "    print(\"Objective function output: \", f(xs))\n",
    "    print(\"Constraint function 1 output: \", g1(xs))\n",
    "    print(\"Constraint function 2 output: \", g2(xs))\n",
    "    print(\"Constraint function 3 output: \", g3(xs))\n",
    "    print(\"Constraint function 4 output: \", g4(xs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SEARCH\n",
    "# [x1,x2](ints 1 <= x <= 99) [x3,x4] (floats 10 <= x <= 200)\n",
    "def random_search(): \n",
    "    K = 3998\n",
    "\n",
    "    x1_best = np.random.randint(low=1, high=99)\n",
    "    x2_best = np.random.randint(low=1, high=99)\n",
    "\n",
    "    x3_best = np.random.uniform(low=10, high=200)\n",
    "    x4_best = np.random.uniform(low=10, high=200)\n",
    "\n",
    "    best_xs = np.array([x1_best, x2_best, x3_best, x4_best])\n",
    "    best_output = f(best_xs)\n",
    "    this_output = 0\n",
    "\n",
    "\n",
    "    for k in range(K):\n",
    "        this_x1 = np.random.randint(low=1, high=99)\n",
    "        this_x2 = np.random.randint(low=1, high=99)\n",
    "\n",
    "        this_x3 = np.random.uniform(low=10, high=200)\n",
    "        this_x4 = np.random.uniform(low=10, high=200)\n",
    "\n",
    "        this_xs = np.array([this_x1, this_x2, this_x3, this_x4])\n",
    "        fx = f(this_xs)\n",
    "        this_output = static_penalty(fx, this_xs)\n",
    "\n",
    "        if (this_output < best_output):\n",
    "            x1_best = this_x1\n",
    "            x2_best = this_x2\n",
    "            x3_best = this_x3\n",
    "            x4_best = this_x4\n",
    "            best_output = this_output\n",
    "\n",
    "            print(f'RS: New best! f(x) = {np.format_float_scientific(best_output)}')\n",
    "            \n",
    "\n",
    "    # print(f\"penalty sum of best RS (infeasible if non-zero): {check_constraints(best_xs)}\")\n",
    "    # print(f\"Best f(x) for RS: {np.format_float_scientific(best_output)}\")\n",
    "    return best_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SEARCH\n",
    "# [x1,x2](ints 1 <= x <= 99) [x3,x4] (floats 10 <= x <= 200)\n",
    "def random_search_death(): \n",
    "    K = 3998\n",
    "\n",
    "    x1_best = np.random.randint(low=1, high=99)\n",
    "    x2_best = np.random.randint(low=1, high=99)\n",
    "\n",
    "    x3_best = np.random.uniform(low=10, high=200)\n",
    "    x4_best = np.random.uniform(low=10, high=200)\n",
    "\n",
    "    best_xs = np.array([x1_best, x2_best, x3_best, x4_best])\n",
    "    best_output = f(best_xs)\n",
    "    this_output = 0\n",
    "\n",
    "\n",
    "    for k in range(K):\n",
    "        this_x1 = np.random.randint(low=1, high=99)\n",
    "        this_x2 = np.random.randint(low=1, high=99)\n",
    "\n",
    "        this_x3 = np.random.uniform(low=10, high=200)\n",
    "        this_x4 = np.random.uniform(low=10, high=200)\n",
    "\n",
    "        this_xs = np.array([this_x1, this_x2, this_x3, this_x4])\n",
    "        fx = f(this_xs)\n",
    "        this_output = death_penalty(fx, this_xs)\n",
    "\n",
    "        if (this_output < best_output):\n",
    "            x1_best = this_x1\n",
    "            x2_best = this_x2\n",
    "            x3_best = this_x3\n",
    "            x4_best = this_x4\n",
    "            best_output = this_output\n",
    "\n",
    "            print(f'RS: New best! f(x) = {np.format_float_scientific(best_output)}')\n",
    "            \n",
    "\n",
    "    # print(f\"penalty sum of best RS (infeasible if non-zero): {check_constraints(best_xs)}\")\n",
    "    # print(f\"Best f(x) for RS: {np.format_float_scientific(best_output)}\")\n",
    "    return best_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Annealing\n",
    "\n",
    "def simulated_annealing():\n",
    "\n",
    "    # Hyper-parameters:\n",
    "    ti = 10\n",
    "    tk = ti\n",
    "    K = 3998\n",
    "    gamma = 0.75\n",
    "\n",
    "    x12_clip_a = 1\n",
    "    x12_clip_b = 99\n",
    "\n",
    "    x34_clip_a = 10\n",
    "    x34_clip_b = 200\n",
    "    x12_bound = (99-1) * 0.3\n",
    "    x34_bound = (200-10) * 0.3\n",
    "\n",
    "    x1_best = round(np.random.uniform(low=1, high=99))\n",
    "    x2_best = round(np.random.uniform(low=1, high=99))\n",
    "    x3_best = np.random.uniform(low=10, high=200)\n",
    "    x4_best = np.random.uniform(low=10, high=200)\n",
    "    best_xs = np.array([x1_best, x2_best, x3_best, x4_best])\n",
    "\n",
    "    y_best = f(best_xs)\n",
    "\n",
    "    xcs = np.array([x1_best,x2_best,x3_best,x4_best])\n",
    "    yc = y_best\n",
    "    \n",
    "    \n",
    "    for k in range(K):\n",
    "\n",
    "        tk = tk*gamma\n",
    "        if (tk == 0):\n",
    "            tk = 0.0000000000001\n",
    "        \n",
    "        x1a, x1b = ( x12_clip_a - xcs[0]) / x12_bound, (x12_clip_b - xcs[0]) / x12_bound\n",
    "        x2a, x2b = ( x12_clip_a - xcs[1]) / x12_bound, (x12_clip_b - xcs[1]) / x12_bound\n",
    "        x3a, x3b = ( x34_clip_a - xcs[2]) / x34_bound, (x34_clip_b - xcs[2]) / x34_bound\n",
    "        x4a, x4b = ( x34_clip_a - xcs[3]) / x34_bound, (x34_clip_b - xcs[3]) / x34_bound\n",
    "\n",
    "        x1p = round(truncnorm.rvs(x1a, x1b, loc=xcs[0], scale=x12_bound))\n",
    "        x2p = round(truncnorm.rvs(x2a, x2b, loc=xcs[1], scale=x12_bound))\n",
    "        x3p = truncnorm.rvs(x3a, x3b, loc=xcs[2], scale=x34_bound)\n",
    "        x4p = truncnorm.rvs(x4a, x4b, loc=xcs[3], scale=x34_bound)\n",
    "\n",
    "        xps = np.array([x1p,x2p,x3p,x4p]) #xps: x-primes\n",
    "\n",
    "        this_y = f(xps)\n",
    "        yp = static_penalty(this_y, xps)\n",
    "\n",
    "        delta_y = (yp-yc)\n",
    "\n",
    "        if(yp - y_best <= 0):\n",
    "            acceptance_prob = 1\n",
    "        \n",
    "        else:\n",
    "            acceptance_prob = math.exp(-1*(delta_y/tk))\n",
    "\n",
    "        if (delta_y <= 0) or (np.random.uniform(low=0.0, high=1.0) < min([acceptance_prob, 1]) ):\n",
    "            xcs = xps\n",
    "            yc = yp\n",
    "        \n",
    "        if (yp < y_best):\n",
    "            best_xs = xps\n",
    "            y_best = yp\n",
    "            print(f'SA: New best! f(x) = {np.format_float_scientific(y_best)}')\n",
    "        \n",
    "\n",
    "    # print(f\"penalty sum of best SA (infeasible if non-zero): {check_constraints(best_xs)}\")\n",
    "    # print(f\"Best xs for SA: {np.array2string(best_xs)}\")\n",
    "    # print(f\"Best xps for SA: {np.array2string(xps)}\")\n",
    "\n",
    "    # print(f\"Best f(x) for SA: {np.format_float_scientific(y_best)}\")\n",
    "    # print(tk)\n",
    "    return y_best\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Annealing\n",
    "\n",
    "def simulated_annealing_death():\n",
    "\n",
    "    # Hyper-parameters:\n",
    "    ti = 10\n",
    "    tk = ti\n",
    "    K = 3998\n",
    "    gamma = 0.75\n",
    "\n",
    "    x12_clip_a = 1\n",
    "    x12_clip_b = 99\n",
    "\n",
    "    x34_clip_a = 10\n",
    "    x34_clip_b = 200\n",
    "    x12_bound = (99-1) * 0.3\n",
    "    x34_bound = (200-10) * 0.3\n",
    "\n",
    "    x1_best = round(np.random.uniform(low=1, high=99))\n",
    "    x2_best = round(np.random.uniform(low=1, high=99))\n",
    "    x3_best = np.random.uniform(low=10, high=200)\n",
    "    x4_best = np.random.uniform(low=10, high=200)\n",
    "    best_xs = np.array([x1_best, x2_best, x3_best, x4_best])\n",
    "\n",
    "    y_best = f(best_xs)\n",
    "\n",
    "    xcs = np.array([x1_best,x2_best,x3_best,x4_best])\n",
    "    yc = y_best\n",
    "    \n",
    "    \n",
    "    for k in range(K):\n",
    "\n",
    "        tk = tk*gamma\n",
    "        if (tk == 0):\n",
    "            tk = 0.0000000000001\n",
    "        \n",
    "        x1a, x1b = ( x12_clip_a - xcs[0]) / x12_bound, (x12_clip_b - xcs[0]) / x12_bound\n",
    "        x2a, x2b = ( x12_clip_a - xcs[1]) / x12_bound, (x12_clip_b - xcs[1]) / x12_bound\n",
    "        x3a, x3b = ( x34_clip_a - xcs[2]) / x34_bound, (x34_clip_b - xcs[2]) / x34_bound\n",
    "        x4a, x4b = ( x34_clip_a - xcs[3]) / x34_bound, (x34_clip_b - xcs[3]) / x34_bound\n",
    "\n",
    "        x1p = round(truncnorm.rvs(x1a, x1b, loc=xcs[0], scale=x12_bound))\n",
    "        x2p = round(truncnorm.rvs(x2a, x2b, loc=xcs[1], scale=x12_bound))\n",
    "        x3p = truncnorm.rvs(x3a, x3b, loc=xcs[2], scale=x34_bound)\n",
    "        x4p = truncnorm.rvs(x4a, x4b, loc=xcs[3], scale=x34_bound)\n",
    "\n",
    "        xps = np.array([x1p,x2p,x3p,x4p]) #xps: x-primes\n",
    "\n",
    "        this_y = f(xps)\n",
    "        yp = death_penalty(this_y, xps)\n",
    "\n",
    "        delta_y = (yp-yc)\n",
    "\n",
    "        if(yp - y_best <= 0):\n",
    "            acceptance_prob = 1\n",
    "        \n",
    "        else:\n",
    "            acceptance_prob = math.exp(-1*(delta_y/tk))\n",
    "\n",
    "        if (delta_y <= 0) or (np.random.uniform(low=0.0, high=1.0) < min([acceptance_prob, 1]) ):\n",
    "            xcs = xps\n",
    "            yc = yp\n",
    "        \n",
    "        if (yp < y_best):\n",
    "            best_xs = xps\n",
    "            y_best = yp\n",
    "            print(f'SA: New best! f(x) = {np.format_float_scientific(y_best)}')\n",
    "        \n",
    "\n",
    "    # print(f\"penalty sum of best SA (infeasible if non-zero): {check_constraints(best_xs)}\")\n",
    "    # print(f\"Best xs for SA: {np.array2string(best_xs)}\")\n",
    "    # print(f\"Best xps for SA: {np.array2string(xps)}\")\n",
    "\n",
    "    # print(f\"Best f(x) for SA: {np.format_float_scientific(y_best)}\")\n",
    "    # print(tk)\n",
    "    return y_best\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(list1, list2):\n",
    "    stat, p = ttest_ind(list1, list2)\n",
    "    print(stat)\n",
    "    print(p)\n",
    "    if (p<=0.05):\n",
    "        print(\"significant difference!\")\n",
    "    else:\n",
    "        print(\"No significant difference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective function output:  572.77017736475\n",
      "Constraint function 1 output:  0.11140812000000001\n",
      "Constraint function 2 output:  0.22538540000000004\n",
      "Constraint function 3 output:  88.76\n",
      "Constraint function 4 output:  -44699.10187026799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bruceprw/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:58: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA: New best! f(x) = 2.63261125346733e+05\n",
      "SA: New best! f(x) = 1.8052295617840881e+05\n",
      "SA: New best! f(x) = 1.6726320415261795e+05\n",
      "SA: New best! f(x) = 1.6587591237114798e+05\n",
      "SA: New best! f(x) = 1.5439127729470292e+05\n",
      "SA: New best! f(x) = 1.5331074256719218e+05\n",
      "SA: New best! f(x) = 1.1884752196979034e+05\n",
      "SA: New best! f(x) = 1.1434860397463472e+05\n",
      "SA: New best! f(x) = 1.0883087490017642e+05\n",
      "SA: New best! f(x) = 1.051072345869828e+05\n",
      "SA: New best! f(x) = 1.0335901564314464e+05\n",
      "SA: New best! f(x) = 9.500448568300762e+04\n",
      "SA: New best! f(x) = 9.139461045434023e+04\n",
      "SA: New best! f(x) = 8.743331556784979e+04\n",
      "RS: New best! f(x) = 2.0792483951741358e+05\n",
      "RS: New best! f(x) = 1.8394626970257168e+05\n",
      "RS: New best! f(x) = 1.379357161162708e+05\n",
      "RS: New best! f(x) = 1.3564327072795975e+05\n",
      "RS: New best! f(x) = 1.1208877819222478e+05\n",
      "RS: New best! f(x) = 1.0885746779552402e+05\n",
      "RS: New best! f(x) = 1.016366414474001e+05\n",
      "RS: New best! f(x) = 9.251616444223981e+04\n",
      "RS: New best! f(x) = 1.385906291848844e+05\n",
      "RS: New best! f(x) = 1.2592464150319144e+05\n",
      "RS: New best! f(x) = 1.2331845891304075e+05\n",
      "RS: New best! f(x) = 1.0426472515234633e+05\n",
      "RS: New best! f(x) = 9.915690248426503e+04\n",
      "SA: New best! f(x) = 1.200265022535893e+05\n",
      "SA: New best! f(x) = 1.0409013918967986e+05\n",
      "SA: New best! f(x) = 1.0395495244261678e+05\n",
      "SA: New best! f(x) = 1.0305017189104369e+05\n",
      "SA: New best! f(x) = 9.70378122057529e+04\n",
      "SA: New best! f(x) = 9.325856394402712e+04\n",
      "SA: New best! f(x) = 8.625576262312755e+04\n",
      "SA: New best! f(x) = 1.2701246758162651e+05\n",
      "SA: New best! f(x) = 1.2287157078476735e+05\n",
      "SA: New best! f(x) = 1.2132637497683881e+05\n",
      "SA: New best! f(x) = 1.1719727684880956e+05\n",
      "SA: New best! f(x) = 1.0087881652857283e+05\n",
      "SA: New best! f(x) = 1.0023989854154445e+05\n",
      "SA: New best! f(x) = 9.916457716521632e+04\n",
      "SA: New best! f(x) = 9.820160041321142e+04\n",
      "SA: New best! f(x) = 9.512622253550762e+04\n",
      "SA: New best! f(x) = 9.11770823712353e+04\n",
      "RS: New best! f(x) = 1.8925310286440008e+05\n",
      "RS: New best! f(x) = 1.5218251541913804e+05\n",
      "RS: New best! f(x) = 1.4309423936055976e+05\n",
      "RS: New best! f(x) = 1.0980273766810716e+05\n",
      "RS: New best! f(x) = 2.440311350510239e+05\n",
      "RS: New best! f(x) = 1.4423920360530584e+05\n",
      "RS: New best! f(x) = 1.1664558076864865e+05\n",
      "RS: New best! f(x) = 1.1137707029745876e+05\n",
      "RS: New best! f(x) = 1.0673562853454157e+05\n",
      "RS: New best! f(x) = 1.0254969837226279e+05\n",
      "RS: New best! f(x) = 1.039114918739507e+05\n",
      "RS: New best! f(x) = 1.0308992122284987e+05\n",
      "RS: New best! f(x) = 9.709924829668947e+04\n",
      "SA: New best! f(x) = 1.4759160675291202e+05\n",
      "SA: New best! f(x) = 1.1199166169572006e+05\n",
      "SA: New best! f(x) = 1.031349410858025e+05\n",
      "SA: New best! f(x) = 1.0190920969307749e+05\n",
      "SA: New best! f(x) = 9.939035804129523e+04\n",
      "SA: New best! f(x) = 9.63787892625317e+04\n",
      "SA: New best! f(x) = 8.912886610505286e+04\n",
      "SA: New best! f(x) = 8.612785238411496e+04\n",
      "SA: New best! f(x) = 1.5485191939301568e+05\n",
      "SA: New best! f(x) = 1.4172954100897032e+05\n",
      "SA: New best! f(x) = 1.280387246231281e+05\n",
      "SA: New best! f(x) = 1.0867418710231136e+05\n",
      "SA: New best! f(x) = 8.936631884044551e+04\n",
      "SA: New best! f(x) = 8.790363085203661e+04\n",
      "SA: New best! f(x) = 8.617250501529868e+04\n",
      "SA: New best! f(x) = 1.4914121070923898e+05\n",
      "SA: New best! f(x) = 1.216898727581074e+05\n",
      "SA: New best! f(x) = 1.1739460788810585e+05\n",
      "SA: New best! f(x) = 9.958702997010338e+04\n",
      "SA: New best! f(x) = 9.09956152000132e+04\n",
      "SA: New best! f(x) = 8.835419879868027e+04\n",
      "SA: New best! f(x) = 8.8066424099049e+04\n",
      "SA: New best! f(x) = 8.382346098144507e+04\n",
      "RS: New best! f(x) = 2.0124853330454772e+05\n",
      "RS: New best! f(x) = 1.866598612940334e+05\n",
      "RS: New best! f(x) = 1.3477526640120576e+05\n",
      "RS: New best! f(x) = 1.2040389896262542e+05\n",
      "RS: New best! f(x) = 1.1138992687325447e+05\n",
      "RS: New best! f(x) = 1.0952770577196027e+05\n",
      "RS: New best! f(x) = 1.0866546478107104e+05\n",
      "SA: New best! f(x) = 1.2125429006007689e+05\n",
      "SA: New best! f(x) = 1.1022085483007193e+05\n",
      "SA: New best! f(x) = 9.331241508192655e+04\n",
      "SA: New best! f(x) = 8.866216344274614e+04\n",
      "SA: New best! f(x) = 8.562549128944891e+04\n",
      "SA: New best! f(x) = 4.36856072569486e+05\n",
      "SA: New best! f(x) = 3.9295667433924635e+05\n",
      "SA: New best! f(x) = 1.808077719487701e+05\n",
      "SA: New best! f(x) = 1.783680065954108e+05\n",
      "SA: New best! f(x) = 1.6099390386921936e+05\n",
      "SA: New best! f(x) = 1.54195005072419e+05\n",
      "SA: New best! f(x) = 1.5084670065841632e+05\n",
      "SA: New best! f(x) = 1.3620854619259093e+05\n",
      "SA: New best! f(x) = 1.0986644707853124e+05\n",
      "SA: New best! f(x) = 9.774571236704283e+04\n",
      "SA: New best! f(x) = 9.633287054896037e+04\n",
      "SA: New best! f(x) = 9.442318303601647e+04\n",
      "SA: New best! f(x) = 9.240144965003237e+04\n",
      "SA: New best! f(x) = 9.130805661945468e+04\n",
      "SA: New best! f(x) = 8.748252543802203e+04\n",
      "SA: New best! f(x) = 8.662014180610653e+04\n",
      "SA: New best! f(x) = 8.480139644162195e+04\n",
      "RS: New best! f(x) = 1.0225951953102472e+05\n",
      "RS: New best! f(x) = 9.177201403160718e+04\n",
      "RS: New best! f(x) = 8.679978907364432e+04\n",
      "SA: New best! f(x) = 1.4637736215131136e+05\n",
      "SA: New best! f(x) = 1.3739022826749968e+05\n",
      "SA: New best! f(x) = 1.2847163691935196e+05\n",
      "SA: New best! f(x) = 1.047495516372374e+05\n",
      "SA: New best! f(x) = 1.0300196122972658e+05\n",
      "SA: New best! f(x) = 1.0237109531810437e+05\n",
      "SA: New best! f(x) = 1.0112932283161547e+05\n",
      "SA: New best! f(x) = 1.0066114446260949e+05\n",
      "SA: New best! f(x) = 8.720628816053912e+04\n",
      "RS: New best! f(x) = 9.884684165375151e+04\n",
      "RS: New best! f(x) = 9.722702117426743e+04\n",
      "RS: New best! f(x) = 9.708194087734913e+04\n",
      "RS: New best! f(x) = 8.94123414656366e+04\n",
      "SA: New best! f(x) = 1.4067386699024428e+05\n",
      "SA: New best! f(x) = 1.3535026559927317e+05\n",
      "SA: New best! f(x) = 1.2770104052973403e+05\n",
      "SA: New best! f(x) = 1.1718241957150909e+05\n",
      "SA: New best! f(x) = 1.0904177191928364e+05\n",
      "SA: New best! f(x) = 9.77017492399638e+04\n",
      "SA: New best! f(x) = 9.621994934148213e+04\n",
      "SA: New best! f(x) = 9.268365769508392e+04\n",
      "SA: New best! f(x) = 8.92679123992337e+04\n",
      "SA: New best! f(x) = 8.657488956277112e+04\n",
      "SA: New best! f(x) = 8.453670981632451e+04\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T test compaing rs to SA (static)\n",
      "1.2383952883710707\n",
      "0.22278839376109746\n",
      "No significant difference!\n",
      "T test compaing rs to SA (death)\n",
      "-0.040823467283712986\n",
      "0.9676397206332981\n",
      "No significant difference!\n",
      "T test compaing rs (static) to rs (death)\n",
      "-2.245267274809355\n",
      "0.030346380768532882\n",
      "significant difference!\n",
      "T test compaing sa (static) to sa (death)\n",
      "-3.0131077805846003\n",
      "0.004470613011656686\n",
      "significant difference!\n"
     ]
    }
   ],
   "source": [
    "rs_list = []\n",
    "sa_list = []\n",
    "rs_death_list = []\n",
    "sa_death_list = []\n",
    "main()\n",
    "# print(\"\\n\")\n",
    "# random_search()\n",
    "# print(\"\\n\")\n",
    "# simulated_annealing()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(21):\n",
    "    rs_out = random_search()\n",
    "    sa_out = simulated_annealing()\n",
    "    rs_list.append(rs_out)\n",
    "    sa_list.append(sa_out)\n",
    "\n",
    "for i in range(21):\n",
    "    rs_out = random_search_death()\n",
    "    sa_out = simulated_annealing_death()\n",
    "    rs_death_list.append(rs_out)\n",
    "    sa_death_list.append(sa_out)\n",
    "\n",
    "plt.boxplot([rs_list, rs_death_list, sa_list, sa_death_list])\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.xlabel(\"Random Search (static(1)/death(2)) & Simulated Annealing (static(3)/death(4))\")\n",
    "plt.show()\n",
    "\n",
    "print(\"T test compaing rs to SA (static)\")\n",
    "stats(rs_list, sa_list)\n",
    "print(\"T test compaing rs to SA (death)\")\n",
    "stats(rs_death_list, sa_death_list)\n",
    "print(\"T test compaing rs (static) to rs (death)\")\n",
    "stats(rs_list, rs_death_list)\n",
    "print(\"T test compaing sa (static) to sa (death)\")\n",
    "stats(sa_list, sa_death_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Results\n",
    "For all my optimisations (Random search and Simulated Annealing using both static penalty and death penalty constraint handling), My results were roughly one factor of ten out from the minimum feasible value.\n",
    "\n",
    "* Optimisers that used static penalties had a much tighter grouping compared to Optimisers that used death penalties, I expect this is because static penalties are better suited towards honing in on local minima.\n",
    "\n",
    "* Statisical Analysis (T test) reveals that there is no significant difference between values obtained from Random Search vs Simulated Annealing when using the same penalty functions.\n",
    "\n",
    "* However there is a sigificant difference between results of the same optimiser that use different penalty functions (ie, RS static and RS death yield significantly different results, and the same for SA.)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
  },
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
